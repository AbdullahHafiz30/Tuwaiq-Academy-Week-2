{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5749a10a",
   "metadata": {},
   "source": [
    "# Weekly Project 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d44841",
   "metadata": {},
   "source": [
    "## Introduction to Road Traffic Accidents (RTA) Dataset\n",
    "\n",
    "### Dataset Overview\n",
    "The RTA Dataset provides a detailed snapshot of road traffic accidents, capturing a range of data from accident conditions to casualty details. This dataset is essential for analyzing patterns and causes of accidents to improve road safety.\n",
    "\n",
    "### Data Characteristics\n",
    "- **Entries**: The dataset contains 12,316 entries.\n",
    "- **Features**: There are 32 features in the dataset, which include:\n",
    "  - `Time`: Time when the accident occurred.\n",
    "  - `Day_of_week`: Day of the week.\n",
    "  - `Age_band_of_driver`: Age group of the driver involved.\n",
    "  - `Sex_of_driver`: Gender of the driver.\n",
    "  - `Educational_level`: Educational level of the driver.\n",
    "  - `Type_of_vehicle`: Type of vehicle involved in the accident.\n",
    "  - `Cause_of_accident`: Reported cause of the accident.\n",
    "  - `Accident_severity`: Severity of the accident.\n",
    "- **Target Column**: `Accident_severity` is used as the target column for modeling. This feature classifies the severity of each accident.\n",
    "\n",
    "### Objective\n",
    "Students will use this dataset to apply various data visualization, modeling, and evaluation techniques learned in class. The primary goal is to build models that can accurately predict the severity of accidents and to identify the key factors that contribute to severe accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177291c2",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Import all the necessary libraries here. Include libraries for handling data (like pandas), visualization (like matplotlib and seaborn), and modeling (like scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2bd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f236af2",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load the dataset from the provided CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f76e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9215c2f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Perform EDA to understand the data better. This involves several steps to summarize the main characteristics, uncover patterns, and establish relationships:\n",
    "* Find the dataset information and observe the datatypes.\n",
    "* Check the shape of the data to understand its structure.\n",
    "* View the the data with various functions to get an initial sense of the data.\n",
    "* Perform summary statistics on the dataset to grasp central tendencies and variability.\n",
    "* Check for duplicated data.\n",
    "* Check for null values.\n",
    "\n",
    "And apply more if needed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206f836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bccfb706",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is essential for transforming raw data into a format suitable for further analysis and modeling. Follow these steps to ensure your data is ready for predictive modeling or advanced analytics:\n",
    "- **Handling Missing Values**: Replace missing values with appropriate statistics (mean, median, mode) or use more complex imputation techniques.\n",
    "- **Normalization/Scaling**: Scale data to a small, specified range like 0 to 1, or transform it to have a mean of zero and a standard deviation of one.\n",
    "- **Label Encoding**: Convert categorical text data into model-understandable numbers where the labels are ordered.\n",
    "- **One-Hot Encoding**: Use for nominal categorical data where no ordinal relationship exists to transform the data into a binary column for each category. (Be careful not to increase the dimensionality significantly)\n",
    "- **Detection and Treatment of Outliers**: Use statistical tests, box plots, or scatter plots to identify outliers and then cap, trim, or use robust methods to reduce the effect of outliers, depending on the context.\n",
    "- **Feature Engineering**: Enhance your dataset by creating new features and transforming existing ones. This might involve combining data from different columns, applying transformations, or reducing dimensionality with techniques like PCA to improve model performance.\n",
    "\n",
    "Consider these steps as a foundation, and feel free to introduce additional preprocessing techniques as needed to address specific characteristics of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4a6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292ab9c9",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Create various plots to visualize the relationships in the data. Consider using the following to show different aspects of the data:\n",
    "\n",
    "* Heatmap of Correlation Matrix.\n",
    "* Line plots.\n",
    "* Scatter plots.\n",
    "* Histograms.\n",
    "* Boxplots.\n",
    "\n",
    "Use more if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa25ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40ce1d71",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "- Choose features that you believe will most influence the outcome based on your analysis and the insights from your visualizations. Focus on those that appear most impactful to include in your modeling.\n",
    "\n",
    "## Train-Test Split\n",
    "* Divide the dataset into training and testing sets to evaluate the performance of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6570748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e74a5058",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Once the data is split into training and testing sets, the next step is to build models to make predictions. Here, we will explore several machine learning algorithms, each with its unique characteristics and suitability for different types of data and problems. You will implement the following models:\n",
    "\n",
    "### 1. Logistic Regression\n",
    "\n",
    "### 2. Decision Tree Classifier\n",
    "\n",
    "### 3. Support Vector Machine (SVM)\n",
    "\n",
    "### 4. K-Neighbors Classifier\n",
    "\n",
    "### Implementing the Models\n",
    "- For each model, use the training data you have prepared to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c9815",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35137d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cfd5ecb",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ce21487",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f448f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3656932",
   "metadata": {},
   "source": [
    "#### K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c73d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12057ba0",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training your models, it's crucial to evaluate their performance to understand their effectiveness and limitations. This section outlines various techniques and metrics to assess the performance of each model you have implemented.\n",
    "\n",
    "### Evaluation Techniques\n",
    "1. **Confusion Matrix**\n",
    "\n",
    "2. **Accuracy**\n",
    "\n",
    "3. **Precision and Recall**\n",
    "\n",
    "4. **F1 Score**\n",
    "\n",
    "5. **ROC Curve and AUC**\n",
    "\n",
    "### Implementing Evaluation\n",
    "- Calculate the metrics listed above using your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8328067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f17c12",
   "metadata": {},
   "source": [
    "## Project Questions:\n",
    "\n",
    "### Comparative Analysis\n",
    "\n",
    "- **Compare Metrics**: Examine the performance metrics (such as accuracy, precision, and recall) of each model. Document your observations on which model performs best for your dataset and the problem you're addressing.\n",
    "- **Evaluate Trade-offs**: Discuss the trade-offs you encountered when choosing between models. Consider factors like computational efficiency, ease of implementation, and model interpretability.\n",
    "- **Justify Your Choice**: After comparing and evaluating, explain why you believe one model is the best choice. Provide a clear rationale based on the performance metrics and trade-offs discussed.\n",
    "- **Feature Importance**: Identify and discuss the most important features for the best-performing model. How do these features impact the predictions? Use the visualizations you have created to justify your answer if necessary.\n",
    "- **Model Limitations**: Discuss any limitations you encountered with the models you used. Are there any aspects of the data or the problem that these models do not handle well?\n",
    "- **Future Improvements**: Suggest potential improvements or further steps you could take to enhance model performance. This could include trying different algorithms, feature engineering techniques, or tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76425529",
   "metadata": {},
   "source": [
    "### Answer Here:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
